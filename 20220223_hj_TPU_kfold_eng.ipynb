{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/KLUE/blob/main/20220223_hj_TPU_kfold_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W2Q9Qbnlz_t"
      },
      "source": [
        "# A full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCra33qIlz_1"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee9BbI2znxA0",
        "outputId": "6899c56c-8fc4-427d-ca20-6836c8dacff3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31KhLlUBlz_2",
        "outputId": "5329b6bd-6205-4262-e1ef-4242da96e2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 80.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 80.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 101.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 88.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 95.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 90.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 79.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 95.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 90.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 85.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Installing collected packages: multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, huggingface-hub, fsspec, aiohttp, xxhash, transformers, sentencepiece, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.3 frozenlist-1.3.0 fsspec-2022.2.0 huggingface-hub-0.4.0 multidict-6.0.2 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.5 transformers-4.16.2 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.10.0.2)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.5.1\n",
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 29 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.10.0.2)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.54.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.10.8)\n",
            "Installing collected packages: google-api-python-client, torch-xla, torch, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.10\n",
            "    Uninstalling google-api-python-client-1.12.10:\n",
            "      Successfully uninstalled google-api-python-client-1.12.10\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "_vxmupBjnvqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModel,AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
        "import gc\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_metric\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from accelerate import notebook_launcher"
      ],
      "metadata": {
        "id": "R7yZowtOokD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_json('/content/drive/MyDrive/KLUE/multinli_1.0_train.jsonl',lines=True)\n",
        "train = train[['sentence1', 'sentence2', 'gold_label']]\n",
        "train.rename(columns = {'sentence1':'premise','sentence2':'hypothesis','gold_label':'label'},inplace=True)\n",
        "train = train.reset_index(drop=True)\n",
        "train['index'] = train.index\n",
        "train = train.dropna()\n",
        "train = train[:100000]"
      ],
      "metadata": {
        "id": "PpIow7U5n0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGWbl8edlz_5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'roberta-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=False)\n",
        "\n",
        "# tokenized_train = tokenizer(\n",
        "#     list(train_dataset['premise']),\n",
        "#     list(train_dataset['hypothesis']),\n",
        "#     return_tensors=\"pt\",\n",
        "#     max_length=128, # Max_Length = 190\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     add_special_tokens=True\n",
        "# )\n",
        "\n",
        "# tokenized_eval = tokenizer(\n",
        "#     list(eval_dataset['premise']),\n",
        "#     list(eval_dataset['hypothesis']),\n",
        "#     return_tensors=\"pt\",\n",
        "#     max_length=128,\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     add_special_tokens=True\n",
        "# )"
      ],
      "metadata": {
        "id": "L32MUBMXocy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['labels'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "J3OrqrY3or6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_num(label):\n",
        "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(label_dict[v])\n",
        "    \n",
        "    return num_label\n",
        "\n",
        "# train_label = label_to_num(train['label'].values)\n",
        "# eval_label = label_to_num(eval_dataset['label'].values)"
      ],
      "metadata": {
        "id": "bqgw3AKZotnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "# eval_dataset = BERTDataset(tokenized_eval, eval_label)"
      ],
      "metadata": {
        "id": "O9oWnK57owKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset[0]"
      ],
      "metadata": {
        "id": "V-LGu7eI9cDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "bs0VFrWuA_6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djc4iH0nl0AA"
      },
      "outputs": [],
      "source": [
        "# def training_function():\n",
        "#   accelerator = Accelerator()\n",
        "\n",
        "#   train_dataloader = torch.utils.data.DataLoader(\n",
        "#                     train_dataset, \n",
        "#                     batch_size=16, sampler=train_subsampler, num_workers=2)\n",
        "#   eval_dataloader = torch.utils.data.DataLoader(\n",
        "#                     train_dataset,\n",
        "#                     batch_size=16, sampler=test_subsampler, num_workers=2)\n",
        "\n",
        "#   config = AutoConfig.from_pretrained(checkpoint)\n",
        "#   config.num_labels = 3\n",
        "#   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "#   optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "#   model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "#   num_epochs = 5\n",
        "#   num_training_steps = num_epochs * len(train_dataloader)\n",
        "#   progress_bar = tqdm(range(num_training_steps))\n",
        "#   lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "#       optimizer=optimizer,\n",
        "#       num_warmup_steps=1,\n",
        "#       num_training_steps=num_training_steps,\n",
        "#   )\n",
        "\n",
        "#   for epoch in range(num_epochs):\n",
        "#       train_acc = 0.0\n",
        "#       test_acc = 0.0\n",
        "\n",
        "#       model.train()\n",
        "#       for batch_id, batch in enumerate(train_dataloader):\n",
        "#           outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "#           loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "#           accelerator.backward(loss)\n",
        "\n",
        "#           optimizer.step()\n",
        "#           lr_scheduler.step()\n",
        "#           optimizer.zero_grad()\n",
        "#           progress_bar.update(1)\n",
        "#           train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "#       print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "#       model.eval()\n",
        "#       for batch_id, batch in enumerate(eval_dataloader):\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "\n",
        "#         test_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "#       print(\"epoch {} test acc {}\".format(epoch+1, test_acc / (batch_id+1)))\n",
        "#       gc.collect()\n",
        "#   accelerator.wait_for_everyone()\n",
        "#   unwrapped_model = accelerator.unwrap_model(model)\n",
        "#   accelerator.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/KLUE/model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# # Start print\n",
        "# # print('--------------------------------')\n",
        "\n",
        "# # K-fold Cross Validation model evaluation\n",
        "# def training_function():\n",
        "#   kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "#   print('--------------------------------')\n",
        "\n",
        "#   for fold, (train_ids, test_ids) in enumerate(kfold.split(train, train['label'])):\n",
        "#     # print('')\n",
        "#     print(f'FOLD {fold}')\n",
        "\n",
        "#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "#     test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "\n",
        "#     train_label = label_to_num(train['label'].values)\n",
        "\n",
        "#     tokenized_train = tokenizer(\n",
        "#       list(train['premise']),\n",
        "#       list(train['hypothesis']),\n",
        "#       return_tensors=\"pt\",\n",
        "#       max_length=128, # Max_Length = 190\n",
        "#       padding=True,\n",
        "#       truncation=True,\n",
        "#       add_special_tokens=True\n",
        "#       )\n",
        "    \n",
        "#     train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "\n",
        "#     # def training_function():\n",
        "#     accelerator = Accelerator()\n",
        "\n",
        "#     train_dataloader = torch.utils.data.DataLoader(\n",
        "#                       train_dataset, \n",
        "#                       batch_size=8, sampler=train_subsampler)\n",
        "#     eval_dataloader = torch.utils.data.DataLoader(\n",
        "#                       train_dataset,\n",
        "#                       batch_size=8, sampler=test_subsampler)\n",
        "\n",
        "#     config = AutoConfig.from_pretrained(checkpoint)\n",
        "#     config.num_labels = 3\n",
        "#     model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "#     optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "#     model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "#     num_epochs = 5\n",
        "#     num_training_steps = num_epochs * len(train_dataloader)\n",
        "#     progress_bar = tqdm(range(num_training_steps))\n",
        "#     lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "#         optimizer=optimizer,\n",
        "#         num_warmup_steps=1,\n",
        "#         num_training_steps=num_training_steps,\n",
        "#     )\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_acc = 0.0\n",
        "#         test_acc = 0.0\n",
        "\n",
        "#         model.train()\n",
        "#         for batch_id, batch in enumerate(train_dataloader):\n",
        "#             outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "#             loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "#             accelerator.backward(loss)\n",
        "\n",
        "#             optimizer.step()\n",
        "#             lr_scheduler.step()\n",
        "#             optimizer.zero_grad()\n",
        "#             progress_bar.update(1)\n",
        "#             train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "#         print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "#         model.eval()\n",
        "#         for batch_id, batch in enumerate(eval_dataloader):\n",
        "#           with torch.no_grad():\n",
        "#               outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "\n",
        "#           test_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "#         print(\"epoch {} test acc {}\".format(epoch+1, test_acc / (batch_id+1)))\n",
        "#         gc.collect()\n",
        "#     accelerator.wait_for_everyone()\n",
        "#     unwrapped_model = accelerator.unwrap_model(model)\n",
        "#     accelerator.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/KLUE/model' + str(fold) + '.pt')"
      ],
      "metadata": {
        "id": "iw581lfmF_J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_function():\n",
        "  accelerator = Accelerator()\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset, \n",
        "                    batch_size=16, sampler=train_subsampler)\n",
        "  eval_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset,\n",
        "                    batch_size=16, sampler=test_subsampler)\n",
        "\n",
        "  config = AutoConfig.from_pretrained(checkpoint)\n",
        "  config.num_labels = 3\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "  model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "  num_epochs = 5\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "      optimizer=optimizer,\n",
        "      num_warmup_steps=1,\n",
        "      num_training_steps=num_training_steps,\n",
        "  )\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      train_acc = 0.0\n",
        "      test_acc = 0.0\n",
        "\n",
        "      model.train()\n",
        "      for batch_id, batch in enumerate(train_dataloader):\n",
        "          outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "          loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "          accelerator.backward(loss)\n",
        "\n",
        "          optimizer.step()\n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        "          train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "      print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "      model.eval()\n",
        "      for batch_id, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "\n",
        "        test_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "      print(\"epoch {} test acc {}\".format(epoch+1, test_acc / (batch_id+1)))\n",
        "      gc.collect()\n",
        "  accelerator.wait_for_everyone()\n",
        "  unwrapped_model = accelerator.unwrap_model(model)\n",
        "  accelerator.save(unwrapped_model.state_dict(), '/content/model/model' + str(fold) + '.pt')"
      ],
      "metadata": {
        "id": "miLIkS4ZQnxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# Start print\n",
        "# print('--------------------------------')\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "print('--------------------------------')\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "  list(train['premise']),\n",
        "  list(train['hypothesis']),\n",
        "  return_tensors=\"pt\",\n",
        "  max_length=128, # Max_Length = 190\n",
        "  padding=True,\n",
        "  truncation=True,\n",
        "  add_special_tokens=True\n",
        "  )\n",
        "  \n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train, train['label'])):\n",
        "  # print('')\n",
        "  print(f'FOLD {fold}')\n",
        "\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "\n",
        "  train_label = label_to_num(train['label'].values)\n",
        "\n",
        "  train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "\n",
        "  notebook_launcher(training_function)"
      ],
      "metadata": {
        "id": "vJ937XNoPln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_variable)"
      ],
      "metadata": {
        "id": "X6tFCrhaIC-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "config.num_labels = 3\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)"
      ],
      "metadata": {
        "id": "xje6ZDJJkmZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerator()\n",
        "model = accelerator.unwrap_model(model)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model/model.pt'))"
      ],
      "metadata": {
        "id": "zHuUP_vQc116"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "efirdTEK_c9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = label_to_num(test['label'].values)\n",
        "\n",
        "tokenized_test = tokenizer(\n",
        "    list(test['premise']),\n",
        "    list(test['hypothesis']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "test_dataset = BERTDataset(tokenized_test, test_label)"
      ],
      "metadata": {
        "id": "XI5RYcMl_eYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "output_pred = []\n",
        "output_prob = []\n",
        "\n",
        "model, dataloader= accelerator.prepare(model, dataloader)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(tqdm(dataloader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=data['input_ids'],\n",
        "            attention_mask=data['attention_mask'],\n",
        "            token_type_ids=data['token_type_ids']\n",
        "        )\n",
        "    logits = outputs[0]\n",
        "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits, axis=-1)\n",
        "\n",
        "    output_pred.append(result)\n",
        "    output_prob.append(prob)\n",
        "  \n",
        "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
        "print(pred_answer)"
      ],
      "metadata": {
        "id": "DLTqv229_gRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_label(label):\n",
        "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
        "    str_label = []\n",
        "\n",
        "    for i, v in enumerate(label):\n",
        "        str_label.append([i,label_dict[v]])\n",
        "    \n",
        "    return str_label\n",
        "\n",
        "answer = num_to_label(pred_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "NABEttQw_h3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/KLUE/submission.csv', index=False)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "6rV5-ld6_i74"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "A full training",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}