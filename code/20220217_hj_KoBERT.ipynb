{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Relation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/KLUE/blob/main/20220217_hj_KoBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face를 활용한 Modeling\n",
        "Google Colab과 Pytorch, Hugging Face Trainer를 사용하여 Model Fine-tuning과 Inference를 진행해보자"
      ],
      "metadata": {
        "id": "suHpaAp_X52A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA(Exploratory Data Analysis)\n",
        "간단하게 Train, Test Dataset의 구조와 문장 분포를 확인해보았습니다."
      ],
      "metadata": {
        "id": "PsWVN24mCsUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Package\n",
        "데이터 확인을 위한 패키지 불러오기"
      ],
      "metadata": {
        "id": "jpY-hSPzYDpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JeyMWkWvX4q2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOVdzwo3CxZX",
        "outputId": "431d3c6f-b09b-477f-ec04-c29522bfc17c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data\n",
        "Data가 위치한 PATH에서 Data를 불러오기"
      ],
      "metadata": {
        "id": "vvKsvMWUDE85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ldTRvy0bXqCz",
        "outputId": "f5ab7fa9-4aa1-4d00-ccbb-34b8c0c26e1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e0bd6531-17f6-46cb-a0cf-d57515344be4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
              "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
              "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
              "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>그들은 부모님을 보고 웃고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>아이들이 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0bd6531-17f6-46cb-a0cf-d57515344be4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0bd6531-17f6-46cb-a0cf-d57515344be4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0bd6531-17f6-46cb-a0cf-d57515344be4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    sentence1                  sentence2     gold_label\n",
              "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n",
              "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n",
              "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.     entailment\n",
              "3          카메라에 웃고 손을 흔드는 아이들          그들은 부모님을 보고 웃고 있다        neutral\n",
              "4          카메라에 웃고 손을 흔드는 아이들                    아이들이 있다     entailment"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "PATH =  '/content/drive/MyDrive/KLUE'\n",
        "\n",
        "train =  pd.read_csv('/content/drive/MyDrive/KorNLUDatasets-master/KorNLI/snli_1.0_train.ko.tsv', sep='\\t')\n",
        "test = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
        "\n",
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path = '/content/drive/MyDrive/KorNLUDatasets-master/KorNLI/'\n",
        "# file = 'multinli.train.ko.tsv'\n",
        "# f = open(path+file,'rt')\n",
        "# reader = csv.reader(f)\n",
        "\n",
        "# csv_list=[]\n",
        "# for l in reader:\n",
        "#   csv_list.append(l)\n",
        "# f.close()\n",
        "\n",
        "# df = pd.DataFrame(csv_list)"
      ],
      "metadata": {
        "id": "BDhE5imaUnY3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, Test Data 확인"
      ],
      "metadata": {
        "id": "JnlEP5U0DQmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.info(), end='\\n\\n')\n",
        "print(test.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaS09wEdrBqK",
        "outputId": "84df06a8-2321-4888-bc01-c64ec2e0e2d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 550152 entries, 0 to 550151\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count   Dtype \n",
            "---  ------      --------------   ----- \n",
            " 0   sentence1   550152 non-null  object\n",
            " 1   sentence2   550146 non-null  object\n",
            " 2   gold_label  550152 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 12.6+ MB\n",
            "None\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1666 entries, 0 to 1665\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   index       1666 non-null   int64 \n",
            " 1   premise     1666 non-null   object\n",
            " 2   hypothesis  1666 non-null   object\n",
            " 3   label       1666 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 52.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.drop(['index'],axis=1)\n",
        "test.rename(columns = {'premise':'sentence1','hypothesis':'sentence2','label':'gold_label'},inplace=True)\n",
        "test"
      ],
      "metadata": {
        "id": "FIDXodzCW71V",
        "outputId": "144ee4ec-e51d-4df1-f0a7-f775739e4a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9482f687-9cf0-4534-bf49-e58f8f43bc14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>다만 조금 좁아서 케리어를 펼치기 불편합니다.</td>\n",
              "      <td>케리어를 펼치기에 공간이 충분했습니다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그리고 위치가 시먼역보다는 샤오난먼역에 가까워요</td>\n",
              "      <td>시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.</td>\n",
              "      <td>무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>몇 번을 다시봐도 볼 때마다 가슴이 저민다.</td>\n",
              "      <td>다시 봤을때는 무덤덤했다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.</td>\n",
              "      <td>8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1661</th>\n",
              "      <td>또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...</td>\n",
              "      <td>이미지 데이터를 가공하는 것이 가장 난이도가 높다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1662</th>\n",
              "      <td>결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.</td>\n",
              "      <td>결말을 보니 분명히 2편이 나올것이 틀림없다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1663</th>\n",
              "      <td>사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...</td>\n",
              "      <td>사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664</th>\n",
              "      <td>로마에서 3박4일간 이곳에서 머물렀습니다.</td>\n",
              "      <td>이곳에서 머무르며 로마의 명소들을 방문했습니다.</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1665</th>\n",
              "      <td>난 당신이 떠날때 길 하나도 못 건넜는데</td>\n",
              "      <td>난 당신이 떤나 사실도 모르고 있었는데</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1666 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9482f687-9cf0-4534-bf49-e58f8f43bc14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9482f687-9cf0-4534-bf49-e58f8f43bc14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9482f687-9cf0-4534-bf49-e58f8f43bc14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              sentence1  ... gold_label\n",
              "0                             다만 조금 좁아서 케리어를 펼치기 불편합니다.  ...     answer\n",
              "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요  ...     answer\n",
              "2                     구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다.  ...     answer\n",
              "3                              몇 번을 다시봐도 볼 때마다 가슴이 저민다.  ...     answer\n",
              "4          8월 중에 입주신청을 하면 청년은 9월, 신혼부부는 10월부터 입주가 가능하다.  ...     answer\n",
              "...                                                 ...  ...        ...\n",
              "1661  또 작업자의 숙련도와 경험 향상, 전문성을 요구하는 난이도 높은 데이터 가공을 통해...  ...     answer\n",
              "1662                   결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다.  ...     answer\n",
              "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...  ...     answer\n",
              "1664                            로마에서 3박4일간 이곳에서 머물렀습니다.  ...     answer\n",
              "1665                             난 당신이 떠날때 길 하나도 못 건넜는데  ...     answer\n",
              "\n",
              "[1666 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Columns: ', train.columns)\n",
        "print('Test Columns: ', test.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvSnwwf-DWyT",
        "outputId": "cd61416b-9ce5-4a36-a2fa-692de800ece7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Columns:  Index(['sentence1', 'sentence2', 'gold_label'], dtype='object')\n",
            "Test Columns:  Index(['sentence1', 'sentence2', 'gold_label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Label: ', train['gold_label'].value_counts(), sep='\\n', end='\\n\\n')\n",
        "print('Test Label: ', test['gold_label'].value_counts(), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdVnI8KTDZLJ",
        "outputId": "e40f22f6-3cb8-4839-d765-00fbe086cd6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Label: \n",
            "neutral          183384\n",
            "contradiction    183384\n",
            "entailment       183384\n",
            "Name: gold_label, dtype: int64\n",
            "\n",
            "Test Label: \n",
            "answer    1666\n",
            "Name: gold_label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Null: ', train.isnull().sum(), sep='\\n', end='\\n\\n')\n",
        "print('Test Null: ', test.isnull().sum(), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzHatuecpFmz",
        "outputId": "620904d5-d648-4bd2-a17d-766f5093c839"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Null: \n",
            "sentence1     0\n",
            "sentence2     6\n",
            "gold_label    0\n",
            "dtype: int64\n",
            "\n",
            "Test Null: \n",
            "sentence1     0\n",
            "sentence2     0\n",
            "gold_label    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.dropna()"
      ],
      "metadata": {
        "id": "43SPaTYVZESV",
        "outputId": "826bf165-42ab-4b30-816a-4da688df879c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6177c029-935f-4cf5-b0bc-dcd38655c3fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
              "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
              "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n",
              "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>그들은 부모님을 보고 웃고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>아이들이 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550147</th>\n",
              "      <td>네 명의 더러운 맨발의 아이들.</td>\n",
              "      <td>4명의 아이들이 '가장 깨끗한 발'로 상을 받았다</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550148</th>\n",
              "      <td>네 명의 더러운 맨발의 아이들.</td>\n",
              "      <td>네 명의 노숙자 아이들이 신발을 도둑맞아서 그들의 발이 더러워졌다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550149</th>\n",
              "      <td>한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다.</td>\n",
              "      <td>바디슈트를 입은 남자가 서핑 대회에 참가하고 있다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550150</th>\n",
              "      <td>한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다.</td>\n",
              "      <td>비즈니스 슈트를 입은 남자가 이사회로 향하고 있다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550151</th>\n",
              "      <td>한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다.</td>\n",
              "      <td>아름다운 푸른 물 위에는 바디슈트를 입은 남자가 서핑을 하고 있다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550146 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6177c029-935f-4cf5-b0bc-dcd38655c3fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6177c029-935f-4cf5-b0bc-dcd38655c3fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6177c029-935f-4cf5-b0bc-dcd38655c3fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                    sentence1  ...     gold_label\n",
              "0                  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  ...        neutral\n",
              "1                  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  ...  contradiction\n",
              "2                  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  ...     entailment\n",
              "3                          카메라에 웃고 손을 흔드는 아이들  ...        neutral\n",
              "4                          카메라에 웃고 손을 흔드는 아이들  ...     entailment\n",
              "...                                       ...  ...            ...\n",
              "550147                      네 명의 더러운 맨발의 아이들.  ...  contradiction\n",
              "550148                      네 명의 더러운 맨발의 아이들.  ...        neutral\n",
              "550149  한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다.  ...        neutral\n",
              "550150  한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다.  ...  contradiction\n",
              "550151  한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다.  ...     entailment\n",
              "\n",
              "[550146 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label 분포\n",
        "Train Dataset의 Label 분포를 Bar Chart를 사용하여 시각화"
      ],
      "metadata": {
        "id": "wJqxpG8LDyep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature = train['gold_label']\n",
        "\n",
        "plt.figure(figsize=(10,7.5))\n",
        "plt.title('Label Count', fontsize=20)\n",
        "\n",
        "temp = feature.value_counts()\n",
        "plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n",
        "plt.text(-0.05, temp.values[0]+20, s=temp.values[0])\n",
        "plt.text(0.95, temp.values[1]+20, s=temp.values[1])\n",
        "plt.text(1.95, temp.values[2]+20, s=temp.values[2])\n",
        "\n",
        "plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
        "plt.show() # 그래프 나타내기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "-fe3NNsxqO7t",
        "outputId": "463c6caf-a315-4c0b-b781-a23bb81c5c01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 전제, 가설 길이 확인\n",
        "Train에 존재하는 Premise(전제), Hypothesis(가설)의 길이를 확인하고 이를 통해 Tokenizer의 max_length 설정이 가능"
      ],
      "metadata": {
        "id": "FPMlfDIhD7mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = np.max(train['sentence1'].str.len())\n",
        "min_len = np.min(train['sentence1'].str.len())\n",
        "mean_len = np.mean(train['sentence1'].str.len())\n",
        "\n",
        "print('Max Premise Length: ', max_len)\n",
        "print('Min Premise Length: ', min_len)\n",
        "print('Mean Premise Lenght: ', mean_len, '\\n')\n",
        "\n",
        "max_len = np.max(train['sentence2'].str.len())\n",
        "min_len = np.min(train['sentence2'].str.len())\n",
        "mean_len = np.mean(train['sentence2'].str.len())\n",
        "\n",
        "print('Max Hypothesis Length: ', max_len)\n",
        "print('Min Hypothesis Length: ', min_len)\n",
        "print('Mean Hypothesis Lenght: ', mean_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV2WNMZQpPIy",
        "outputId": "9ccaa983-e124-4fd2-b0a5-639ed7dc3929"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Premise Length:  186\n",
            "Min Premise Length:  2\n",
            "Mean Premise Lenght:  32.205061510273524 \n",
            "\n",
            "Max Hypothesis Length:  148.0\n",
            "Min Hypothesis Length:  0.0\n",
            "Mean Hypothesis Lenght:  18.811135589461706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "plt.figure(figsize=(10,7.5))\n",
        "plt.title('Premise Length', fontsize=20)\n",
        "\n",
        "plt.hist(train['premise'].str.len(), alpha=0.5, color='orange')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "1nQ6JvawJ1B4",
        "outputId": "26fb0e56-4a62-4426-9c9f-cf8bb5096b75"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'premise'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-35a4ca6b0d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Premise Length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'premise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 레이아웃 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'premise'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x540 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 간단한 Test Preprocessing\n",
        "전제, 가설에 존재하는 한글 단어가 아닌 다른 단어들은 전부 제거해줍니다."
      ],
      "metadata": {
        "id": "tj5E7uVAEXoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['sentence1'] = train['sentence1'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
        "test['sentence1'] = test['sentence1'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
        "train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "aBbzHrrfLYuX",
        "outputId": "86ae392a-4996-46e8-d264-ffe36c65bd07"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c02ad50a-cf9a-47eb-a7fe-458b8504f756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>사람은 야외에서 말을 타고 있다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>그들은 부모님을 보고 웃고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>아이들이 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c02ad50a-cf9a-47eb-a7fe-458b8504f756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c02ad50a-cf9a-47eb-a7fe-458b8504f756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c02ad50a-cf9a-47eb-a7fe-458b8504f756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   sentence1                  sentence2     gold_label\n",
              "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n",
              "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n",
              "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다         사람은 야외에서 말을 타고 있다.     entailment\n",
              "3         카메라에 웃고 손을 흔드는 아이들          그들은 부모님을 보고 웃고 있다        neutral\n",
              "4         카메라에 웃고 손을 흔드는 아이들                    아이들이 있다     entailment"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['sentence2'] = train['sentence2'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
        "test['sentence2'] = test['sentence2'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
        "train.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "MCFpm8BwMfao",
        "outputId": "4e240ee6-879c-440c-ecd7-3cb21348265c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-309b7a97-6b3e-48ff-88b1-5cc77d751c63\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>사람은 야외에서 말을 타고 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>그들은 부모님을 보고 웃고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>아이들이 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-309b7a97-6b3e-48ff-88b1-5cc77d751c63')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-309b7a97-6b3e-48ff-88b1-5cc77d751c63 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-309b7a97-6b3e-48ff-88b1-5cc77d751c63');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                   sentence1                 sentence2     gold_label\n",
              "0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다  한 사람이 경쟁을 위해 말을 훈련시키고 있다        neutral\n",
              "1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다   한 사람이 식당에서 오믈렛을 주문하고 있다  contradiction\n",
              "2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다         사람은 야외에서 말을 타고 있다     entailment\n",
              "3         카메라에 웃고 손을 흔드는 아이들         그들은 부모님을 보고 웃고 있다        neutral\n",
              "4         카메라에 웃고 손을 흔드는 아이들                   아이들이 있다     entailment"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "sTDAsjbeEjIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download transformers and Import Package"
      ],
      "metadata": {
        "id": "Ws3oFvBkEpIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gCodaAYMnHO",
        "outputId": "9a852025-9071-47d0-ca9f-441b9642f3eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
      ],
      "metadata": {
        "id": "6Xhz9OFjEvzm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seed 고정, GPU 설정"
      ],
      "metadata": {
        "id": "Jh16K_pjFS11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed:int = 1004):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwLrRTFkzWYu",
        "outputId": "6152f705-0bee-400c-ebc8-3d1a215079a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Tokenizer, Model\n",
        "Hugging Face Hub에 존재하는 Pretrained Tokenizer와 Model 및 Model Config를 불러옵니다.\n",
        "\n",
        "이 때, Classification은 num_labels가 2로 Default되어있기 때문에 Model Config의 Parameter를 변경해줍니다."
      ],
      "metadata": {
        "id": "cnrEI1X8Faup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'klue/roberta-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = 3\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "print(model)\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59sIma4zTLTh",
        "outputId": "e7c6e1ca-baa9-46e3-8208-1aa2e0059a25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaForSequenceClassification(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (18): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (19): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (20): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (21): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (22): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (23): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): RobertaClassificationHead(\n",
            "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(train)\n",
        "display(test)"
      ],
      "metadata": {
        "id": "TRvwbCOubJsl",
        "outputId": "6a6589da-3240-40c6-ccd2-719e05a9eed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-15b58212-bbcf-40f5-9d4c-0b57d7ee7ab9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>한 사람이 식당에서 오믈렛을 주문하고 있다</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다</td>\n",
              "      <td>사람은 야외에서 말을 타고 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>그들은 부모님을 보고 웃고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>카메라에 웃고 손을 흔드는 아이들</td>\n",
              "      <td>아이들이 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550147</th>\n",
              "      <td>네 명의 더러운 맨발의 아이들</td>\n",
              "      <td>4명의 아이들이 가장 깨끗한 발로 상을 받았다</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550148</th>\n",
              "      <td>네 명의 더러운 맨발의 아이들</td>\n",
              "      <td>네 명의 노숙자 아이들이 신발을 도둑맞아서 그들의 발이 더러워졌다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550149</th>\n",
              "      <td>한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다</td>\n",
              "      <td>바디슈트를 입은 남자가 서핑 대회에 참가하고 있다</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550150</th>\n",
              "      <td>한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다</td>\n",
              "      <td>비즈니스 슈트를 입은 남자가 이사회로 향하고 있다</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550151</th>\n",
              "      <td>한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다</td>\n",
              "      <td>아름다운 푸른 물 위에는 바디슈트를 입은 남자가 서핑을 하고 있다</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550152 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15b58212-bbcf-40f5-9d4c-0b57d7ee7ab9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15b58212-bbcf-40f5-9d4c-0b57d7ee7ab9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15b58212-bbcf-40f5-9d4c-0b57d7ee7ab9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                   sentence1  ...     gold_label\n",
              "0                  말을 탄 사람이 고장난 비행기 위로 뛰어오른다  ...        neutral\n",
              "1                  말을 탄 사람이 고장난 비행기 위로 뛰어오른다  ...  contradiction\n",
              "2                  말을 탄 사람이 고장난 비행기 위로 뛰어오른다  ...     entailment\n",
              "3                         카메라에 웃고 손을 흔드는 아이들  ...        neutral\n",
              "4                         카메라에 웃고 손을 흔드는 아이들  ...     entailment\n",
              "...                                      ...  ...            ...\n",
              "550147                      네 명의 더러운 맨발의 아이들  ...  contradiction\n",
              "550148                      네 명의 더러운 맨발의 아이들  ...        neutral\n",
              "550149  한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다  ...        neutral\n",
              "550150  한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다  ...  contradiction\n",
              "550151  한 남자가 아름다운 푸른 물에서 바디슈트를 입고 서핑을 하고 있다  ...     entailment\n",
              "\n",
              "[550152 rows x 3 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-69af12da-a059-4c26-84af-614eff47568d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>gold_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>다만 조금 좁아서 케리어를 펼치기 불편합니다</td>\n",
              "      <td>케리어를 펼치기에 공간이 충분했습니다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>그리고 위치가 시먼역보다는 샤오난먼역에 가까워요</td>\n",
              "      <td>시먼역보다는 샤오난먼역에 먼저 도착할 수 있어요</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다</td>\n",
              "      <td>무엇인가 말을 많이 하기는 했지만 큰 의미가 있지는 않았다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>몇 번을 다시봐도 볼 때마다 가슴이 저민다</td>\n",
              "      <td>다시 봤을때는 무덤덤했다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8월 중에 입주신청을 하면 청년은 9월 신혼부부는 10월부터 입주가 가능하다</td>\n",
              "      <td>8월 중에 입주신청을 하면 신혼부부는 9월 부터 입주가 가능하다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1661</th>\n",
              "      <td>또 작업자의 숙련도와 경험 향상 전문성을 요구하는 난이도 높은 데이터 가공을 통해 ...</td>\n",
              "      <td>이미지 데이터를 가공하는 것이 가장 난이도가 높다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1662</th>\n",
              "      <td>결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다</td>\n",
              "      <td>결말을 보니 분명히 2편이 나올것이 틀림없다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1663</th>\n",
              "      <td>사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...</td>\n",
              "      <td>사회적 거리두기 상황에서는 고위관직자도 방역지침을 준수해야 한다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664</th>\n",
              "      <td>로마에서 3박4일간 이곳에서 머물렀습니다</td>\n",
              "      <td>이곳에서 머무르며 로마의 명소들을 방문했습니다</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1665</th>\n",
              "      <td>난 당신이 떠날때 길 하나도 못 건넜는데</td>\n",
              "      <td>난 당신이 떤나 사실도 모르고 있었는데</td>\n",
              "      <td>answer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1666 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69af12da-a059-4c26-84af-614eff47568d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69af12da-a059-4c26-84af-614eff47568d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69af12da-a059-4c26-84af-614eff47568d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              sentence1  ... gold_label\n",
              "0                              다만 조금 좁아서 케리어를 펼치기 불편합니다  ...     answer\n",
              "1                            그리고 위치가 시먼역보다는 샤오난먼역에 가까워요  ...     answer\n",
              "2                      구구절절 설명하고 이해시키려는 노력이 큰 의미없이 다가온다  ...     answer\n",
              "3                               몇 번을 다시봐도 볼 때마다 가슴이 저민다  ...     answer\n",
              "4            8월 중에 입주신청을 하면 청년은 9월 신혼부부는 10월부터 입주가 가능하다  ...     answer\n",
              "...                                                 ...  ...        ...\n",
              "1661  또 작업자의 숙련도와 경험 향상 전문성을 요구하는 난이도 높은 데이터 가공을 통해 ...  ...     answer\n",
              "1662                    결말을 보니 아무래도 이 영화는 2부가 계획된 듯 합니다  ...     answer\n",
              "1663  사회적 거리 두기 상황에서 총리도 카페를 갔다가 자리가 없어서 퇴짜 맞은 일도 있을...  ...     answer\n",
              "1664                             로마에서 3박4일간 이곳에서 머물렀습니다  ...     answer\n",
              "1665                             난 당신이 떠날때 길 하나도 못 건넜는데  ...     answer\n",
              "\n",
              "[1666 rows x 3 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n",
        "Train Data를 Train과 Validation Dataset으로 나누고 각각 데이터를 Tokenizer를 통해 Tokenizing을 합니다.\n",
        "\n",
        "Tokenizer에 들어가는 문장은 전제와 가설을 Concat한 문장이 됩니다."
      ],
      "metadata": {
        "id": "_G-UXaLSF5g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['gold_label'])\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "    list(train['sentence1']),\n",
        "    list(train['sentence2']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128, # Max_Length = 190\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "tokenized_eval = tokenizer(\n",
        "    list(eval_dataset['sentence1']),\n",
        "    list(eval_dataset['sentence2']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "print(tokenized_train['input_ids'][0])\n",
        "print(tokenizer.decode(tokenized_train['input_ids'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "uuVioFwzqK_E",
        "outputId": "bc748c75-5e42-46f8-cced-b20a1fd1c6f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4bf73f8481d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2458\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2460\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2461\u001b[0m             )\n\u001b[1;32m   2462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m         )\n\u001b[1;32m   2653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mis_pretokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         )\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['gold_label'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "QGD8NaEKsjd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_num(label):\n",
        "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(label_dict[v])\n",
        "    \n",
        "    return num_label\n",
        "\n",
        "\n",
        "train_label = label_to_num(train_dataset['gold_label'].values)\n",
        "eval_label = label_to_num(eval_dataset['gold_label'].values)"
      ],
      "metadata": {
        "id": "A__OJJds4bT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
        "\n",
        "print(train_dataset.__len__())\n",
        "print(train_dataset.__getitem__(19997))\n",
        "print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))"
      ],
      "metadata": {
        "id": "LX6KzqVc6WcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b58ac68-2f87-49da-e9b6-484a7d4b8288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19998\n",
            "{'input_ids': tensor([    0,  4081,  2170,  1513,  2414,  9713,  2031,  2145,  5293,  2031,\n",
            "         2073, 11217,  2371,  2062,     2,  4081,  2170,  1513,  2414,  9713,\n",
            "         2031,  2073, 11217,  2371,  2062,     2,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]), 'label': tensor(0)}\n",
            "[CLS] 현장에 있던 제작진들과 관객들은 환호했다 [SEP] 현장에 있던 제작진들은 환호했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "  \"\"\" validation을 위한 metrics function \"\"\"\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  probs = pred.predictions\n",
        "\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
        "\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ],
      "metadata": {
        "id": "ad-aYSAm7InR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_ars = TrainingArguments(\n",
        "    output_dir='./result',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_total_limit=5,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps = 500,\n",
        "    load_best_model_at_end = True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_ars,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "6zLkAk4A98dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "model.save_pretrained('./result/best_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "mMKDknOd_7y2",
        "outputId": "5b4ac58b-af07-4243-9a3b-3b1343098751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 19998\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='79' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  79/6250 01:16 < 1:42:22, 1.00 it/s, Epoch 0.12/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "Tokenizer_NAME = \"klue/roberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
        "\n",
        "MODEL_NAME = './result/checkpoint-5000'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.resize_token_embeddings(tokenizer.vocab_size)\n",
        "model.to(device)\n",
        "\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okQ3vBMrp1S-",
        "outputId": "3c719a24-c0d0-4ada-cade-e3dfc27e2ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
            "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/360b579947002f14f22331a026821b56f70679f1be1e95fe5dc5a80edc4a59e0.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
            "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
            "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n",
            "loading configuration file ./result/checkpoint-5000/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"./result/checkpoint-5000\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file ./result/checkpoint-5000/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
            "\n",
            "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./result/checkpoint-5000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = label_to_num(test['label'].values)\n",
        "\n",
        "tokenized_test = tokenizer(\n",
        "    list(test['premise']),\n",
        "    list(test['hypothesis']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "test_dataset = BERTDataset(tokenized_test, test_label)\n",
        "\n",
        "print(test_dataset.__len__())\n",
        "print(test_dataset.__getitem__(1665))\n",
        "print(tokenizer.decode(test_dataset.__getitem__(6)['input_ids']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocAbx6PW20B9",
        "outputId": "d42e24c2-753b-4a71-c7fb-68ce7bee7219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1666\n",
            "{'input_ids': tensor([    0,   720,  3994,  2052, 10428,  2775,   647,  3657,  2119,  1085,\n",
            "            3,     2,   720,  3994,  2052,   911,  2075,  3669,  2119,  3926,\n",
            "         2088,  1513,  2359, 13964,     2,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
            "[CLS] 18일 귀국이라 발인도 지켜드리지 못해 더욱 죄송할 따름입니다 [SEP] 18일 배를 타고 여행을 떠났습니다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "output_pred = []\n",
        "output_prob = []\n",
        "\n",
        "for i, data in enumerate(tqdm(dataloader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=data['input_ids'].to(device),\n",
        "            attention_mask=data['attention_mask'].to(device),\n",
        "            token_type_ids=data['token_type_ids'].to(device)\n",
        "        )\n",
        "    logits = outputs[0]\n",
        "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits, axis=-1)\n",
        "\n",
        "    output_pred.append(result)\n",
        "    output_prob.append(prob)\n",
        "  \n",
        "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
        "print(pred_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrTbY3NRq1eC",
        "outputId": "53c9927c-359a-42e2-e13d-5071b72cc915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:22<00:00,  4.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 0, 1, 1, 2, 2, 0, 0, 1, 1, 0, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 0, 2, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 1, 0, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1, 2, 1, 0, 2, 0, 1, 2, 1, 1, 2, 1, 2, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0, 0, 0, 2, 1, 2, 2, 0, 2, 0, 2, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0, 1, 2, 0, 1, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 1, 1, 0, 0, 2, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1, 0, 2, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 2, 2, 2, 0, 2, 2, 1, 1, 0, 1, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 2, 2, 0, 1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 0, 2, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 0, 1, 2, 0, 1, 2, 2, 1, 2, 2, 0, 0, 2, 1, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 1, 2, 0, 2, 2, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 2, 0, 2, 0, 2, 2, 1, 1, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 0, 1, 2, 1, 0, 0, 1, 1, 0, 2, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 2, 1, 2, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 1, 0, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 2, 0, 2, 2, 1, 0, 1, 1, 2, 0, 2, 0, 0, 2, 1, 2, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 2, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 2, 2, 1, 2, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 1, 0, 2, 0, 2, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 1, 1, 2, 0, 1, 1, 2, 2, 2, 0, 1, 0, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 1, 1, 2, 1, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 1, 0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 1, 2, 2, 0, 2, 1, 0, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 1, 2, 1, 2, 0, 0, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 0, 0, 1, 2, 0, 1, 0, 2, 0, 2, 2, 0, 1, 0, 0, 2, 0, 1, 2, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 1, 0, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0, 0, 2, 1, 2, 1, 2, 0, 0, 2, 1, 0, 1, 2, 2, 2, 2, 1, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 1, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 2, 1, 1, 0, 1, 0, 2, 0, 0, 1, 1, 2, 0, 0, 2, 2, 0, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 2, 2, 2, 1, 1, 0, 2, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 2, 2, 0, 0, 1, 0, 1, 0, 0, 2, 2, 2, 1, 0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 0, 2, 1, 1, 1, 1, 1, 2, 0, 1, 2, 0, 2, 1, 1, 2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 2, 0, 1, 0, 1, 2, 0, 1, 2, 2, 0, 2, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 0, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 2, 0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0, 2, 1, 2, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 1, 2, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 2, 0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 0, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 2, 1, 2, 2, 0, 2, 1, 1, 0, 2, 1, 1, 2, 0, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 1, 0, 2, 0, 1, 1, 0, 1, 2, 2, 2, 2, 0, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 1, 2, 2, 0, 1, 0, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2, 2, 1, 2, 2, 0, 2, 2, 1, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 2, 1, 0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 2, 2, 0, 2, 2, 2, 0, 1, 1, 0, 1, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 1, 2, 1, 1, 0, 2, 1, 1, 0, 1, 2, 0, 1, 2, 2, 2, 2, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 0, 0, 1, 2, 2, 0, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 2, 2, 1, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 1, 2, 1, 2, 1, 2, 0, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 1, 0, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 0, 2, 2, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_label(label):\n",
        "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
        "    str_label = []\n",
        "\n",
        "    for i, v in enumerate(label):\n",
        "        str_label.append([i,label_dict[v]])\n",
        "    \n",
        "    return str_label\n",
        "\n",
        "answer = num_to_label(pred_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHgoD__nyOiC",
        "outputId": "5d6faddb-cb13-4bad-e77f-736b0fec7763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 'contradiction'], [1, 'neutral'], [2, 'entailment'], [3, 'contradiction'], [4, 'contradiction'], [5, 'neutral'], [6, 'neutral'], [7, 'entailment'], [8, 'entailment'], [9, 'contradiction'], [10, 'contradiction'], [11, 'entailment'], [12, 'contradiction'], [13, 'neutral'], [14, 'neutral'], [15, 'neutral'], [16, 'neutral'], [17, 'neutral'], [18, 'contradiction'], [19, 'neutral'], [20, 'contradiction'], [21, 'neutral'], [22, 'contradiction'], [23, 'entailment'], [24, 'contradiction'], [25, 'contradiction'], [26, 'entailment'], [27, 'entailment'], [28, 'entailment'], [29, 'neutral'], [30, 'contradiction'], [31, 'entailment'], [32, 'contradiction'], [33, 'neutral'], [34, 'neutral'], [35, 'contradiction'], [36, 'entailment'], [37, 'contradiction'], [38, 'entailment'], [39, 'contradiction'], [40, 'neutral'], [41, 'entailment'], [42, 'entailment'], [43, 'neutral'], [44, 'neutral'], [45, 'contradiction'], [46, 'neutral'], [47, 'entailment'], [48, 'contradiction'], [49, 'entailment'], [50, 'entailment'], [51, 'contradiction'], [52, 'entailment'], [53, 'neutral'], [54, 'contradiction'], [55, 'entailment'], [56, 'neutral'], [57, 'neutral'], [58, 'neutral'], [59, 'neutral'], [60, 'entailment'], [61, 'neutral'], [62, 'neutral'], [63, 'neutral'], [64, 'neutral'], [65, 'contradiction'], [66, 'neutral'], [67, 'neutral'], [68, 'neutral'], [69, 'contradiction'], [70, 'entailment'], [71, 'neutral'], [72, 'entailment'], [73, 'neutral'], [74, 'neutral'], [75, 'neutral'], [76, 'neutral'], [77, 'neutral'], [78, 'neutral'], [79, 'contradiction'], [80, 'neutral'], [81, 'contradiction'], [82, 'contradiction'], [83, 'contradiction'], [84, 'entailment'], [85, 'contradiction'], [86, 'neutral'], [87, 'contradiction'], [88, 'entailment'], [89, 'neutral'], [90, 'contradiction'], [91, 'neutral'], [92, 'entailment'], [93, 'contradiction'], [94, 'entailment'], [95, 'neutral'], [96, 'contradiction'], [97, 'contradiction'], [98, 'entailment'], [99, 'neutral'], [100, 'neutral'], [101, 'entailment'], [102, 'neutral'], [103, 'entailment'], [104, 'neutral'], [105, 'entailment'], [106, 'neutral'], [107, 'neutral'], [108, 'contradiction'], [109, 'entailment'], [110, 'contradiction'], [111, 'neutral'], [112, 'contradiction'], [113, 'entailment'], [114, 'neutral'], [115, 'contradiction'], [116, 'contradiction'], [117, 'contradiction'], [118, 'contradiction'], [119, 'contradiction'], [120, 'contradiction'], [121, 'contradiction'], [122, 'contradiction'], [123, 'contradiction'], [124, 'contradiction'], [125, 'entailment'], [126, 'entailment'], [127, 'entailment'], [128, 'contradiction'], [129, 'neutral'], [130, 'contradiction'], [131, 'neutral'], [132, 'contradiction'], [133, 'entailment'], [134, 'neutral'], [135, 'entailment'], [136, 'contradiction'], [137, 'neutral'], [138, 'contradiction'], [139, 'contradiction'], [140, 'neutral'], [141, 'contradiction'], [142, 'neutral'], [143, 'entailment'], [144, 'contradiction'], [145, 'neutral'], [146, 'entailment'], [147, 'contradiction'], [148, 'neutral'], [149, 'neutral'], [150, 'contradiction'], [151, 'entailment'], [152, 'contradiction'], [153, 'neutral'], [154, 'entailment'], [155, 'entailment'], [156, 'entailment'], [157, 'neutral'], [158, 'contradiction'], [159, 'neutral'], [160, 'neutral'], [161, 'entailment'], [162, 'neutral'], [163, 'entailment'], [164, 'neutral'], [165, 'contradiction'], [166, 'neutral'], [167, 'entailment'], [168, 'neutral'], [169, 'entailment'], [170, 'neutral'], [171, 'neutral'], [172, 'contradiction'], [173, 'neutral'], [174, 'contradiction'], [175, 'neutral'], [176, 'neutral'], [177, 'neutral'], [178, 'contradiction'], [179, 'neutral'], [180, 'entailment'], [181, 'neutral'], [182, 'neutral'], [183, 'neutral'], [184, 'neutral'], [185, 'neutral'], [186, 'contradiction'], [187, 'neutral'], [188, 'entailment'], [189, 'contradiction'], [190, 'neutral'], [191, 'entailment'], [192, 'contradiction'], [193, 'entailment'], [194, 'neutral'], [195, 'entailment'], [196, 'neutral'], [197, 'neutral'], [198, 'entailment'], [199, 'neutral'], [200, 'neutral'], [201, 'neutral'], [202, 'entailment'], [203, 'contradiction'], [204, 'contradiction'], [205, 'entailment'], [206, 'entailment'], [207, 'neutral'], [208, 'entailment'], [209, 'neutral'], [210, 'contradiction'], [211, 'contradiction'], [212, 'contradiction'], [213, 'neutral'], [214, 'contradiction'], [215, 'contradiction'], [216, 'contradiction'], [217, 'contradiction'], [218, 'contradiction'], [219, 'contradiction'], [220, 'neutral'], [221, 'neutral'], [222, 'entailment'], [223, 'neutral'], [224, 'neutral'], [225, 'contradiction'], [226, 'contradiction'], [227, 'neutral'], [228, 'contradiction'], [229, 'contradiction'], [230, 'entailment'], [231, 'neutral'], [232, 'entailment'], [233, 'entailment'], [234, 'entailment'], [235, 'neutral'], [236, 'neutral'], [237, 'contradiction'], [238, 'entailment'], [239, 'entailment'], [240, 'entailment'], [241, 'contradiction'], [242, 'neutral'], [243, 'neutral'], [244, 'neutral'], [245, 'entailment'], [246, 'neutral'], [247, 'neutral'], [248, 'contradiction'], [249, 'contradiction'], [250, 'entailment'], [251, 'contradiction'], [252, 'neutral'], [253, 'entailment'], [254, 'contradiction'], [255, 'neutral'], [256, 'contradiction'], [257, 'entailment'], [258, 'entailment'], [259, 'entailment'], [260, 'entailment'], [261, 'entailment'], [262, 'contradiction'], [263, 'neutral'], [264, 'neutral'], [265, 'neutral'], [266, 'contradiction'], [267, 'neutral'], [268, 'neutral'], [269, 'entailment'], [270, 'contradiction'], [271, 'neutral'], [272, 'entailment'], [273, 'entailment'], [274, 'neutral'], [275, 'contradiction'], [276, 'contradiction'], [277, 'contradiction'], [278, 'neutral'], [279, 'neutral'], [280, 'neutral'], [281, 'entailment'], [282, 'neutral'], [283, 'entailment'], [284, 'contradiction'], [285, 'entailment'], [286, 'contradiction'], [287, 'contradiction'], [288, 'entailment'], [289, 'neutral'], [290, 'entailment'], [291, 'contradiction'], [292, 'entailment'], [293, 'contradiction'], [294, 'entailment'], [295, 'contradiction'], [296, 'contradiction'], [297, 'contradiction'], [298, 'neutral'], [299, 'entailment'], [300, 'neutral'], [301, 'entailment'], [302, 'neutral'], [303, 'entailment'], [304, 'contradiction'], [305, 'entailment'], [306, 'neutral'], [307, 'contradiction'], [308, 'entailment'], [309, 'entailment'], [310, 'neutral'], [311, 'contradiction'], [312, 'contradiction'], [313, 'neutral'], [314, 'neutral'], [315, 'neutral'], [316, 'neutral'], [317, 'neutral'], [318, 'entailment'], [319, 'entailment'], [320, 'entailment'], [321, 'neutral'], [322, 'neutral'], [323, 'entailment'], [324, 'neutral'], [325, 'contradiction'], [326, 'contradiction'], [327, 'neutral'], [328, 'contradiction'], [329, 'contradiction'], [330, 'neutral'], [331, 'entailment'], [332, 'entailment'], [333, 'contradiction'], [334, 'contradiction'], [335, 'contradiction'], [336, 'entailment'], [337, 'contradiction'], [338, 'contradiction'], [339, 'entailment'], [340, 'contradiction'], [341, 'neutral'], [342, 'neutral'], [343, 'contradiction'], [344, 'neutral'], [345, 'entailment'], [346, 'contradiction'], [347, 'contradiction'], [348, 'contradiction'], [349, 'neutral'], [350, 'contradiction'], [351, 'neutral'], [352, 'entailment'], [353, 'contradiction'], [354, 'neutral'], [355, 'contradiction'], [356, 'neutral'], [357, 'entailment'], [358, 'contradiction'], [359, 'neutral'], [360, 'entailment'], [361, 'contradiction'], [362, 'neutral'], [363, 'neutral'], [364, 'contradiction'], [365, 'neutral'], [366, 'neutral'], [367, 'entailment'], [368, 'entailment'], [369, 'neutral'], [370, 'contradiction'], [371, 'neutral'], [372, 'entailment'], [373, 'neutral'], [374, 'neutral'], [375, 'entailment'], [376, 'entailment'], [377, 'neutral'], [378, 'entailment'], [379, 'entailment'], [380, 'entailment'], [381, 'neutral'], [382, 'contradiction'], [383, 'neutral'], [384, 'entailment'], [385, 'neutral'], [386, 'neutral'], [387, 'entailment'], [388, 'neutral'], [389, 'contradiction'], [390, 'neutral'], [391, 'entailment'], [392, 'contradiction'], [393, 'neutral'], [394, 'contradiction'], [395, 'contradiction'], [396, 'neutral'], [397, 'neutral'], [398, 'contradiction'], [399, 'neutral'], [400, 'contradiction'], [401, 'neutral'], [402, 'contradiction'], [403, 'contradiction'], [404, 'entailment'], [405, 'neutral'], [406, 'contradiction'], [407, 'neutral'], [408, 'contradiction'], [409, 'neutral'], [410, 'contradiction'], [411, 'contradiction'], [412, 'contradiction'], [413, 'contradiction'], [414, 'neutral'], [415, 'contradiction'], [416, 'contradiction'], [417, 'entailment'], [418, 'contradiction'], [419, 'contradiction'], [420, 'neutral'], [421, 'neutral'], [422, 'entailment'], [423, 'neutral'], [424, 'entailment'], [425, 'neutral'], [426, 'neutral'], [427, 'contradiction'], [428, 'contradiction'], [429, 'contradiction'], [430, 'entailment'], [431, 'neutral'], [432, 'contradiction'], [433, 'neutral'], [434, 'contradiction'], [435, 'neutral'], [436, 'entailment'], [437, 'entailment'], [438, 'entailment'], [439, 'entailment'], [440, 'entailment'], [441, 'contradiction'], [442, 'neutral'], [443, 'neutral'], [444, 'contradiction'], [445, 'neutral'], [446, 'neutral'], [447, 'entailment'], [448, 'contradiction'], [449, 'neutral'], [450, 'contradiction'], [451, 'entailment'], [452, 'entailment'], [453, 'contradiction'], [454, 'contradiction'], [455, 'entailment'], [456, 'neutral'], [457, 'contradiction'], [458, 'neutral'], [459, 'contradiction'], [460, 'entailment'], [461, 'entailment'], [462, 'contradiction'], [463, 'contradiction'], [464, 'neutral'], [465, 'contradiction'], [466, 'contradiction'], [467, 'neutral'], [468, 'entailment'], [469, 'neutral'], [470, 'contradiction'], [471, 'neutral'], [472, 'entailment'], [473, 'contradiction'], [474, 'entailment'], [475, 'neutral'], [476, 'entailment'], [477, 'contradiction'], [478, 'entailment'], [479, 'contradiction'], [480, 'entailment'], [481, 'entailment'], [482, 'contradiction'], [483, 'entailment'], [484, 'neutral'], [485, 'entailment'], [486, 'entailment'], [487, 'neutral'], [488, 'contradiction'], [489, 'entailment'], [490, 'entailment'], [491, 'neutral'], [492, 'neutral'], [493, 'entailment'], [494, 'entailment'], [495, 'entailment'], [496, 'entailment'], [497, 'neutral'], [498, 'neutral'], [499, 'entailment'], [500, 'entailment'], [501, 'entailment'], [502, 'neutral'], [503, 'contradiction'], [504, 'contradiction'], [505, 'entailment'], [506, 'neutral'], [507, 'contradiction'], [508, 'entailment'], [509, 'contradiction'], [510, 'neutral'], [511, 'entailment'], [512, 'entailment'], [513, 'entailment'], [514, 'entailment'], [515, 'contradiction'], [516, 'entailment'], [517, 'entailment'], [518, 'contradiction'], [519, 'contradiction'], [520, 'entailment'], [521, 'neutral'], [522, 'contradiction'], [523, 'neutral'], [524, 'neutral'], [525, 'contradiction'], [526, 'entailment'], [527, 'entailment'], [528, 'contradiction'], [529, 'entailment'], [530, 'neutral'], [531, 'neutral'], [532, 'contradiction'], [533, 'neutral'], [534, 'neutral'], [535, 'contradiction'], [536, 'contradiction'], [537, 'contradiction'], [538, 'neutral'], [539, 'neutral'], [540, 'neutral'], [541, 'contradiction'], [542, 'contradiction'], [543, 'entailment'], [544, 'contradiction'], [545, 'neutral'], [546, 'contradiction'], [547, 'entailment'], [548, 'neutral'], [549, 'entailment'], [550, 'neutral'], [551, 'contradiction'], [552, 'entailment'], [553, 'entailment'], [554, 'entailment'], [555, 'neutral'], [556, 'entailment'], [557, 'neutral'], [558, 'neutral'], [559, 'contradiction'], [560, 'entailment'], [561, 'contradiction'], [562, 'contradiction'], [563, 'neutral'], [564, 'entailment'], [565, 'neutral'], [566, 'entailment'], [567, 'entailment'], [568, 'neutral'], [569, 'contradiction'], [570, 'neutral'], [571, 'entailment'], [572, 'entailment'], [573, 'neutral'], [574, 'entailment'], [575, 'contradiction'], [576, 'entailment'], [577, 'entailment'], [578, 'contradiction'], [579, 'contradiction'], [580, 'contradiction'], [581, 'entailment'], [582, 'entailment'], [583, 'neutral'], [584, 'entailment'], [585, 'entailment'], [586, 'neutral'], [587, 'entailment'], [588, 'contradiction'], [589, 'entailment'], [590, 'entailment'], [591, 'neutral'], [592, 'contradiction'], [593, 'entailment'], [594, 'contradiction'], [595, 'contradiction'], [596, 'contradiction'], [597, 'neutral'], [598, 'entailment'], [599, 'entailment'], [600, 'contradiction'], [601, 'neutral'], [602, 'neutral'], [603, 'entailment'], [604, 'neutral'], [605, 'neutral'], [606, 'contradiction'], [607, 'contradiction'], [608, 'contradiction'], [609, 'contradiction'], [610, 'contradiction'], [611, 'entailment'], [612, 'entailment'], [613, 'entailment'], [614, 'contradiction'], [615, 'contradiction'], [616, 'neutral'], [617, 'neutral'], [618, 'contradiction'], [619, 'neutral'], [620, 'contradiction'], [621, 'neutral'], [622, 'entailment'], [623, 'contradiction'], [624, 'entailment'], [625, 'contradiction'], [626, 'contradiction'], [627, 'contradiction'], [628, 'neutral'], [629, 'entailment'], [630, 'contradiction'], [631, 'entailment'], [632, 'neutral'], [633, 'entailment'], [634, 'neutral'], [635, 'neutral'], [636, 'contradiction'], [637, 'entailment'], [638, 'entailment'], [639, 'neutral'], [640, 'contradiction'], [641, 'entailment'], [642, 'neutral'], [643, 'entailment'], [644, 'neutral'], [645, 'entailment'], [646, 'neutral'], [647, 'contradiction'], [648, 'contradiction'], [649, 'neutral'], [650, 'entailment'], [651, 'contradiction'], [652, 'contradiction'], [653, 'neutral'], [654, 'neutral'], [655, 'neutral'], [656, 'entailment'], [657, 'contradiction'], [658, 'entailment'], [659, 'contradiction'], [660, 'neutral'], [661, 'contradiction'], [662, 'neutral'], [663, 'neutral'], [664, 'neutral'], [665, 'contradiction'], [666, 'contradiction'], [667, 'neutral'], [668, 'neutral'], [669, 'neutral'], [670, 'neutral'], [671, 'neutral'], [672, 'neutral'], [673, 'contradiction'], [674, 'neutral'], [675, 'entailment'], [676, 'neutral'], [677, 'neutral'], [678, 'contradiction'], [679, 'contradiction'], [680, 'neutral'], [681, 'contradiction'], [682, 'entailment'], [683, 'contradiction'], [684, 'contradiction'], [685, 'contradiction'], [686, 'entailment'], [687, 'neutral'], [688, 'entailment'], [689, 'entailment'], [690, 'neutral'], [691, 'entailment'], [692, 'entailment'], [693, 'neutral'], [694, 'entailment'], [695, 'entailment'], [696, 'entailment'], [697, 'contradiction'], [698, 'neutral'], [699, 'entailment'], [700, 'neutral'], [701, 'entailment'], [702, 'entailment'], [703, 'neutral'], [704, 'entailment'], [705, 'entailment'], [706, 'contradiction'], [707, 'contradiction'], [708, 'neutral'], [709, 'contradiction'], [710, 'entailment'], [711, 'entailment'], [712, 'neutral'], [713, 'entailment'], [714, 'entailment'], [715, 'entailment'], [716, 'neutral'], [717, 'neutral'], [718, 'neutral'], [719, 'entailment'], [720, 'entailment'], [721, 'neutral'], [722, 'neutral'], [723, 'contradiction'], [724, 'entailment'], [725, 'contradiction'], [726, 'neutral'], [727, 'neutral'], [728, 'entailment'], [729, 'entailment'], [730, 'entailment'], [731, 'neutral'], [732, 'entailment'], [733, 'neutral'], [734, 'entailment'], [735, 'neutral'], [736, 'contradiction'], [737, 'neutral'], [738, 'neutral'], [739, 'entailment'], [740, 'neutral'], [741, 'contradiction'], [742, 'entailment'], [743, 'entailment'], [744, 'neutral'], [745, 'neutral'], [746, 'neutral'], [747, 'contradiction'], [748, 'neutral'], [749, 'neutral'], [750, 'neutral'], [751, 'neutral'], [752, 'neutral'], [753, 'entailment'], [754, 'entailment'], [755, 'neutral'], [756, 'entailment'], [757, 'contradiction'], [758, 'neutral'], [759, 'neutral'], [760, 'entailment'], [761, 'neutral'], [762, 'entailment'], [763, 'contradiction'], [764, 'neutral'], [765, 'entailment'], [766, 'neutral'], [767, 'entailment'], [768, 'contradiction'], [769, 'neutral'], [770, 'contradiction'], [771, 'neutral'], [772, 'entailment'], [773, 'entailment'], [774, 'neutral'], [775, 'neutral'], [776, 'contradiction'], [777, 'entailment'], [778, 'neutral'], [779, 'neutral'], [780, 'entailment'], [781, 'neutral'], [782, 'contradiction'], [783, 'neutral'], [784, 'entailment'], [785, 'entailment'], [786, 'contradiction'], [787, 'neutral'], [788, 'entailment'], [789, 'contradiction'], [790, 'entailment'], [791, 'neutral'], [792, 'entailment'], [793, 'neutral'], [794, 'neutral'], [795, 'entailment'], [796, 'contradiction'], [797, 'entailment'], [798, 'entailment'], [799, 'neutral'], [800, 'entailment'], [801, 'contradiction'], [802, 'neutral'], [803, 'neutral'], [804, 'contradiction'], [805, 'contradiction'], [806, 'entailment'], [807, 'entailment'], [808, 'contradiction'], [809, 'entailment'], [810, 'entailment'], [811, 'contradiction'], [812, 'contradiction'], [813, 'neutral'], [814, 'neutral'], [815, 'neutral'], [816, 'neutral'], [817, 'neutral'], [818, 'neutral'], [819, 'entailment'], [820, 'neutral'], [821, 'contradiction'], [822, 'neutral'], [823, 'neutral'], [824, 'neutral'], [825, 'neutral'], [826, 'contradiction'], [827, 'neutral'], [828, 'contradiction'], [829, 'contradiction'], [830, 'entailment'], [831, 'contradiction'], [832, 'contradiction'], [833, 'entailment'], [834, 'neutral'], [835, 'contradiction'], [836, 'entailment'], [837, 'contradiction'], [838, 'neutral'], [839, 'neutral'], [840, 'contradiction'], [841, 'entailment'], [842, 'contradiction'], [843, 'entailment'], [844, 'neutral'], [845, 'neutral'], [846, 'contradiction'], [847, 'contradiction'], [848, 'entailment'], [849, 'neutral'], [850, 'neutral'], [851, 'contradiction'], [852, 'entailment'], [853, 'neutral'], [854, 'contradiction'], [855, 'entailment'], [856, 'entailment'], [857, 'neutral'], [858, 'contradiction'], [859, 'entailment'], [860, 'contradiction'], [861, 'neutral'], [862, 'contradiction'], [863, 'neutral'], [864, 'neutral'], [865, 'entailment'], [866, 'entailment'], [867, 'neutral'], [868, 'contradiction'], [869, 'neutral'], [870, 'contradiction'], [871, 'neutral'], [872, 'entailment'], [873, 'entailment'], [874, 'neutral'], [875, 'contradiction'], [876, 'entailment'], [877, 'contradiction'], [878, 'neutral'], [879, 'neutral'], [880, 'neutral'], [881, 'neutral'], [882, 'contradiction'], [883, 'entailment'], [884, 'entailment'], [885, 'contradiction'], [886, 'neutral'], [887, 'neutral'], [888, 'entailment'], [889, 'neutral'], [890, 'entailment'], [891, 'contradiction'], [892, 'entailment'], [893, 'entailment'], [894, 'contradiction'], [895, 'neutral'], [896, 'entailment'], [897, 'neutral'], [898, 'entailment'], [899, 'entailment'], [900, 'contradiction'], [901, 'contradiction'], [902, 'entailment'], [903, 'entailment'], [904, 'entailment'], [905, 'neutral'], [906, 'contradiction'], [907, 'contradiction'], [908, 'contradiction'], [909, 'entailment'], [910, 'neutral'], [911, 'neutral'], [912, 'neutral'], [913, 'entailment'], [914, 'neutral'], [915, 'neutral'], [916, 'contradiction'], [917, 'contradiction'], [918, 'neutral'], [919, 'entailment'], [920, 'neutral'], [921, 'entailment'], [922, 'entailment'], [923, 'entailment'], [924, 'neutral'], [925, 'contradiction'], [926, 'neutral'], [927, 'entailment'], [928, 'neutral'], [929, 'entailment'], [930, 'neutral'], [931, 'entailment'], [932, 'neutral'], [933, 'neutral'], [934, 'contradiction'], [935, 'contradiction'], [936, 'entailment'], [937, 'contradiction'], [938, 'entailment'], [939, 'neutral'], [940, 'entailment'], [941, 'entailment'], [942, 'contradiction'], [943, 'contradiction'], [944, 'neutral'], [945, 'entailment'], [946, 'entailment'], [947, 'neutral'], [948, 'neutral'], [949, 'entailment'], [950, 'contradiction'], [951, 'neutral'], [952, 'contradiction'], [953, 'contradiction'], [954, 'contradiction'], [955, 'neutral'], [956, 'neutral'], [957, 'entailment'], [958, 'contradiction'], [959, 'contradiction'], [960, 'neutral'], [961, 'contradiction'], [962, 'contradiction'], [963, 'contradiction'], [964, 'entailment'], [965, 'entailment'], [966, 'contradiction'], [967, 'entailment'], [968, 'entailment'], [969, 'contradiction'], [970, 'entailment'], [971, 'entailment'], [972, 'neutral'], [973, 'contradiction'], [974, 'contradiction'], [975, 'entailment'], [976, 'entailment'], [977, 'neutral'], [978, 'entailment'], [979, 'entailment'], [980, 'entailment'], [981, 'neutral'], [982, 'entailment'], [983, 'contradiction'], [984, 'neutral'], [985, 'neutral'], [986, 'contradiction'], [987, 'contradiction'], [988, 'neutral'], [989, 'contradiction'], [990, 'entailment'], [991, 'entailment'], [992, 'neutral'], [993, 'contradiction'], [994, 'neutral'], [995, 'entailment'], [996, 'neutral'], [997, 'contradiction'], [998, 'entailment'], [999, 'entailment'], [1000, 'entailment'], [1001, 'entailment'], [1002, 'entailment'], [1003, 'entailment'], [1004, 'neutral'], [1005, 'contradiction'], [1006, 'entailment'], [1007, 'entailment'], [1008, 'neutral'], [1009, 'neutral'], [1010, 'neutral'], [1011, 'entailment'], [1012, 'neutral'], [1013, 'contradiction'], [1014, 'neutral'], [1015, 'entailment'], [1016, 'entailment'], [1017, 'entailment'], [1018, 'contradiction'], [1019, 'contradiction'], [1020, 'entailment'], [1021, 'neutral'], [1022, 'entailment'], [1023, 'contradiction'], [1024, 'entailment'], [1025, 'neutral'], [1026, 'neutral'], [1027, 'neutral'], [1028, 'contradiction'], [1029, 'contradiction'], [1030, 'entailment'], [1031, 'neutral'], [1032, 'contradiction'], [1033, 'entailment'], [1034, 'neutral'], [1035, 'contradiction'], [1036, 'neutral'], [1037, 'contradiction'], [1038, 'contradiction'], [1039, 'contradiction'], [1040, 'contradiction'], [1041, 'neutral'], [1042, 'contradiction'], [1043, 'entailment'], [1044, 'contradiction'], [1045, 'neutral'], [1046, 'neutral'], [1047, 'entailment'], [1048, 'entailment'], [1049, 'contradiction'], [1050, 'entailment'], [1051, 'contradiction'], [1052, 'entailment'], [1053, 'entailment'], [1054, 'neutral'], [1055, 'neutral'], [1056, 'neutral'], [1057, 'contradiction'], [1058, 'entailment'], [1059, 'neutral'], [1060, 'contradiction'], [1061, 'contradiction'], [1062, 'contradiction'], [1063, 'entailment'], [1064, 'entailment'], [1065, 'neutral'], [1066, 'entailment'], [1067, 'entailment'], [1068, 'entailment'], [1069, 'neutral'], [1070, 'entailment'], [1071, 'entailment'], [1072, 'neutral'], [1073, 'neutral'], [1074, 'entailment'], [1075, 'neutral'], [1076, 'entailment'], [1077, 'neutral'], [1078, 'contradiction'], [1079, 'entailment'], [1080, 'neutral'], [1081, 'contradiction'], [1082, 'contradiction'], [1083, 'contradiction'], [1084, 'contradiction'], [1085, 'contradiction'], [1086, 'neutral'], [1087, 'entailment'], [1088, 'contradiction'], [1089, 'neutral'], [1090, 'entailment'], [1091, 'neutral'], [1092, 'contradiction'], [1093, 'contradiction'], [1094, 'neutral'], [1095, 'neutral'], [1096, 'entailment'], [1097, 'neutral'], [1098, 'neutral'], [1099, 'neutral'], [1100, 'entailment'], [1101, 'contradiction'], [1102, 'entailment'], [1103, 'entailment'], [1104, 'neutral'], [1105, 'entailment'], [1106, 'contradiction'], [1107, 'contradiction'], [1108, 'entailment'], [1109, 'contradiction'], [1110, 'contradiction'], [1111, 'contradiction'], [1112, 'contradiction'], [1113, 'contradiction'], [1114, 'neutral'], [1115, 'neutral'], [1116, 'neutral'], [1117, 'entailment'], [1118, 'entailment'], [1119, 'contradiction'], [1120, 'contradiction'], [1121, 'neutral'], [1122, 'neutral'], [1123, 'contradiction'], [1124, 'neutral'], [1125, 'neutral'], [1126, 'entailment'], [1127, 'contradiction'], [1128, 'entailment'], [1129, 'contradiction'], [1130, 'entailment'], [1131, 'entailment'], [1132, 'entailment'], [1133, 'contradiction'], [1134, 'contradiction'], [1135, 'contradiction'], [1136, 'entailment'], [1137, 'contradiction'], [1138, 'contradiction'], [1139, 'entailment'], [1140, 'entailment'], [1141, 'contradiction'], [1142, 'contradiction'], [1143, 'contradiction'], [1144, 'entailment'], [1145, 'neutral'], [1146, 'entailment'], [1147, 'neutral'], [1148, 'entailment'], [1149, 'contradiction'], [1150, 'entailment'], [1151, 'contradiction'], [1152, 'neutral'], [1153, 'entailment'], [1154, 'contradiction'], [1155, 'neutral'], [1156, 'neutral'], [1157, 'entailment'], [1158, 'neutral'], [1159, 'neutral'], [1160, 'entailment'], [1161, 'contradiction'], [1162, 'contradiction'], [1163, 'contradiction'], [1164, 'neutral'], [1165, 'entailment'], [1166, 'neutral'], [1167, 'entailment'], [1168, 'neutral'], [1169, 'neutral'], [1170, 'contradiction'], [1171, 'neutral'], [1172, 'contradiction'], [1173, 'neutral'], [1174, 'neutral'], [1175, 'entailment'], [1176, 'neutral'], [1177, 'neutral'], [1178, 'neutral'], [1179, 'contradiction'], [1180, 'neutral'], [1181, 'entailment'], [1182, 'contradiction'], [1183, 'neutral'], [1184, 'contradiction'], [1185, 'neutral'], [1186, 'neutral'], [1187, 'neutral'], [1188, 'entailment'], [1189, 'contradiction'], [1190, 'entailment'], [1191, 'entailment'], [1192, 'contradiction'], [1193, 'neutral'], [1194, 'entailment'], [1195, 'entailment'], [1196, 'contradiction'], [1197, 'neutral'], [1198, 'contradiction'], [1199, 'neutral'], [1200, 'neutral'], [1201, 'contradiction'], [1202, 'contradiction'], [1203, 'neutral'], [1204, 'neutral'], [1205, 'neutral'], [1206, 'contradiction'], [1207, 'contradiction'], [1208, 'entailment'], [1209, 'contradiction'], [1210, 'neutral'], [1211, 'entailment'], [1212, 'entailment'], [1213, 'contradiction'], [1214, 'neutral'], [1215, 'contradiction'], [1216, 'neutral'], [1217, 'entailment'], [1218, 'neutral'], [1219, 'neutral'], [1220, 'neutral'], [1221, 'contradiction'], [1222, 'contradiction'], [1223, 'neutral'], [1224, 'entailment'], [1225, 'contradiction'], [1226, 'contradiction'], [1227, 'entailment'], [1228, 'entailment'], [1229, 'neutral'], [1230, 'contradiction'], [1231, 'contradiction'], [1232, 'entailment'], [1233, 'contradiction'], [1234, 'contradiction'], [1235, 'entailment'], [1236, 'entailment'], [1237, 'entailment'], [1238, 'entailment'], [1239, 'neutral'], [1240, 'neutral'], [1241, 'neutral'], [1242, 'neutral'], [1243, 'contradiction'], [1244, 'entailment'], [1245, 'contradiction'], [1246, 'contradiction'], [1247, 'contradiction'], [1248, 'neutral'], [1249, 'entailment'], [1250, 'neutral'], [1251, 'entailment'], [1252, 'neutral'], [1253, 'contradiction'], [1254, 'neutral'], [1255, 'contradiction'], [1256, 'entailment'], [1257, 'entailment'], [1258, 'contradiction'], [1259, 'entailment'], [1260, 'neutral'], [1261, 'entailment'], [1262, 'entailment'], [1263, 'neutral'], [1264, 'neutral'], [1265, 'neutral'], [1266, 'entailment'], [1267, 'contradiction'], [1268, 'contradiction'], [1269, 'contradiction'], [1270, 'neutral'], [1271, 'entailment'], [1272, 'contradiction'], [1273, 'entailment'], [1274, 'contradiction'], [1275, 'neutral'], [1276, 'neutral'], [1277, 'entailment'], [1278, 'contradiction'], [1279, 'entailment'], [1280, 'contradiction'], [1281, 'entailment'], [1282, 'contradiction'], [1283, 'entailment'], [1284, 'neutral'], [1285, 'entailment'], [1286, 'contradiction'], [1287, 'neutral'], [1288, 'entailment'], [1289, 'neutral'], [1290, 'neutral'], [1291, 'neutral'], [1292, 'neutral'], [1293, 'neutral'], [1294, 'neutral'], [1295, 'contradiction'], [1296, 'contradiction'], [1297, 'neutral'], [1298, 'contradiction'], [1299, 'neutral'], [1300, 'contradiction'], [1301, 'entailment'], [1302, 'entailment'], [1303, 'neutral'], [1304, 'neutral'], [1305, 'neutral'], [1306, 'contradiction'], [1307, 'neutral'], [1308, 'entailment'], [1309, 'contradiction'], [1310, 'neutral'], [1311, 'contradiction'], [1312, 'neutral'], [1313, 'neutral'], [1314, 'contradiction'], [1315, 'neutral'], [1316, 'neutral'], [1317, 'entailment'], [1318, 'neutral'], [1319, 'contradiction'], [1320, 'contradiction'], [1321, 'entailment'], [1322, 'neutral'], [1323, 'contradiction'], [1324, 'contradiction'], [1325, 'neutral'], [1326, 'entailment'], [1327, 'neutral'], [1328, 'neutral'], [1329, 'contradiction'], [1330, 'neutral'], [1331, 'neutral'], [1332, 'contradiction'], [1333, 'neutral'], [1334, 'contradiction'], [1335, 'neutral'], [1336, 'contradiction'], [1337, 'neutral'], [1338, 'entailment'], [1339, 'neutral'], [1340, 'entailment'], [1341, 'contradiction'], [1342, 'entailment'], [1343, 'neutral'], [1344, 'entailment'], [1345, 'contradiction'], [1346, 'contradiction'], [1347, 'entailment'], [1348, 'contradiction'], [1349, 'neutral'], [1350, 'neutral'], [1351, 'neutral'], [1352, 'neutral'], [1353, 'entailment'], [1354, 'neutral'], [1355, 'contradiction'], [1356, 'contradiction'], [1357, 'contradiction'], [1358, 'contradiction'], [1359, 'contradiction'], [1360, 'neutral'], [1361, 'contradiction'], [1362, 'neutral'], [1363, 'neutral'], [1364, 'neutral'], [1365, 'neutral'], [1366, 'contradiction'], [1367, 'contradiction'], [1368, 'entailment'], [1369, 'entailment'], [1370, 'neutral'], [1371, 'contradiction'], [1372, 'neutral'], [1373, 'contradiction'], [1374, 'contradiction'], [1375, 'neutral'], [1376, 'neutral'], [1377, 'contradiction'], [1378, 'entailment'], [1379, 'neutral'], [1380, 'contradiction'], [1381, 'neutral'], [1382, 'neutral'], [1383, 'entailment'], [1384, 'contradiction'], [1385, 'entailment'], [1386, 'entailment'], [1387, 'contradiction'], [1388, 'neutral'], [1389, 'contradiction'], [1390, 'neutral'], [1391, 'neutral'], [1392, 'neutral'], [1393, 'entailment'], [1394, 'entailment'], [1395, 'neutral'], [1396, 'entailment'], [1397, 'entailment'], [1398, 'entailment'], [1399, 'neutral'], [1400, 'neutral'], [1401, 'neutral'], [1402, 'entailment'], [1403, 'entailment'], [1404, 'entailment'], [1405, 'contradiction'], [1406, 'entailment'], [1407, 'entailment'], [1408, 'contradiction'], [1409, 'contradiction'], [1410, 'entailment'], [1411, 'neutral'], [1412, 'neutral'], [1413, 'entailment'], [1414, 'contradiction'], [1415, 'neutral'], [1416, 'neutral'], [1417, 'contradiction'], [1418, 'entailment'], [1419, 'entailment'], [1420, 'entailment'], [1421, 'neutral'], [1422, 'contradiction'], [1423, 'entailment'], [1424, 'neutral'], [1425, 'entailment'], [1426, 'entailment'], [1427, 'contradiction'], [1428, 'neutral'], [1429, 'neutral'], [1430, 'contradiction'], [1431, 'neutral'], [1432, 'neutral'], [1433, 'entailment'], [1434, 'neutral'], [1435, 'neutral'], [1436, 'contradiction'], [1437, 'neutral'], [1438, 'neutral'], [1439, 'contradiction'], [1440, 'contradiction'], [1441, 'entailment'], [1442, 'contradiction'], [1443, 'entailment'], [1444, 'neutral'], [1445, 'contradiction'], [1446, 'contradiction'], [1447, 'neutral'], [1448, 'contradiction'], [1449, 'entailment'], [1450, 'entailment'], [1451, 'contradiction'], [1452, 'entailment'], [1453, 'neutral'], [1454, 'entailment'], [1455, 'contradiction'], [1456, 'contradiction'], [1457, 'neutral'], [1458, 'contradiction'], [1459, 'neutral'], [1460, 'contradiction'], [1461, 'neutral'], [1462, 'contradiction'], [1463, 'entailment'], [1464, 'entailment'], [1465, 'contradiction'], [1466, 'contradiction'], [1467, 'neutral'], [1468, 'neutral'], [1469, 'contradiction'], [1470, 'entailment'], [1471, 'contradiction'], [1472, 'neutral'], [1473, 'contradiction'], [1474, 'entailment'], [1475, 'neutral'], [1476, 'neutral'], [1477, 'entailment'], [1478, 'entailment'], [1479, 'contradiction'], [1480, 'contradiction'], [1481, 'contradiction'], [1482, 'contradiction'], [1483, 'neutral'], [1484, 'entailment'], [1485, 'neutral'], [1486, 'neutral'], [1487, 'contradiction'], [1488, 'contradiction'], [1489, 'neutral'], [1490, 'neutral'], [1491, 'entailment'], [1492, 'neutral'], [1493, 'neutral'], [1494, 'neutral'], [1495, 'entailment'], [1496, 'contradiction'], [1497, 'contradiction'], [1498, 'entailment'], [1499, 'contradiction'], [1500, 'contradiction'], [1501, 'neutral'], [1502, 'entailment'], [1503, 'entailment'], [1504, 'neutral'], [1505, 'entailment'], [1506, 'entailment'], [1507, 'entailment'], [1508, 'neutral'], [1509, 'entailment'], [1510, 'entailment'], [1511, 'contradiction'], [1512, 'contradiction'], [1513, 'neutral'], [1514, 'contradiction'], [1515, 'contradiction'], [1516, 'entailment'], [1517, 'neutral'], [1518, 'contradiction'], [1519, 'contradiction'], [1520, 'entailment'], [1521, 'contradiction'], [1522, 'neutral'], [1523, 'entailment'], [1524, 'contradiction'], [1525, 'neutral'], [1526, 'neutral'], [1527, 'neutral'], [1528, 'neutral'], [1529, 'contradiction'], [1530, 'neutral'], [1531, 'entailment'], [1532, 'contradiction'], [1533, 'contradiction'], [1534, 'neutral'], [1535, 'contradiction'], [1536, 'neutral'], [1537, 'entailment'], [1538, 'entailment'], [1539, 'neutral'], [1540, 'contradiction'], [1541, 'contradiction'], [1542, 'neutral'], [1543, 'neutral'], [1544, 'entailment'], [1545, 'contradiction'], [1546, 'neutral'], [1547, 'neutral'], [1548, 'neutral'], [1549, 'neutral'], [1550, 'contradiction'], [1551, 'entailment'], [1552, 'neutral'], [1553, 'neutral'], [1554, 'neutral'], [1555, 'neutral'], [1556, 'entailment'], [1557, 'neutral'], [1558, 'entailment'], [1559, 'entailment'], [1560, 'neutral'], [1561, 'entailment'], [1562, 'neutral'], [1563, 'neutral'], [1564, 'contradiction'], [1565, 'entailment'], [1566, 'contradiction'], [1567, 'entailment'], [1568, 'entailment'], [1569, 'contradiction'], [1570, 'neutral'], [1571, 'neutral'], [1572, 'entailment'], [1573, 'contradiction'], [1574, 'contradiction'], [1575, 'contradiction'], [1576, 'neutral'], [1577, 'contradiction'], [1578, 'contradiction'], [1579, 'neutral'], [1580, 'neutral'], [1581, 'contradiction'], [1582, 'contradiction'], [1583, 'entailment'], [1584, 'neutral'], [1585, 'neutral'], [1586, 'neutral'], [1587, 'neutral'], [1588, 'entailment'], [1589, 'entailment'], [1590, 'entailment'], [1591, 'entailment'], [1592, 'contradiction'], [1593, 'entailment'], [1594, 'neutral'], [1595, 'neutral'], [1596, 'entailment'], [1597, 'entailment'], [1598, 'neutral'], [1599, 'neutral'], [1600, 'contradiction'], [1601, 'contradiction'], [1602, 'contradiction'], [1603, 'neutral'], [1604, 'entailment'], [1605, 'neutral'], [1606, 'entailment'], [1607, 'entailment'], [1608, 'contradiction'], [1609, 'entailment'], [1610, 'neutral'], [1611, 'neutral'], [1612, 'entailment'], [1613, 'neutral'], [1614, 'entailment'], [1615, 'contradiction'], [1616, 'neutral'], [1617, 'contradiction'], [1618, 'neutral'], [1619, 'contradiction'], [1620, 'neutral'], [1621, 'entailment'], [1622, 'contradiction'], [1623, 'neutral'], [1624, 'contradiction'], [1625, 'neutral'], [1626, 'neutral'], [1627, 'entailment'], [1628, 'contradiction'], [1629, 'entailment'], [1630, 'contradiction'], [1631, 'entailment'], [1632, 'entailment'], [1633, 'neutral'], [1634, 'entailment'], [1635, 'neutral'], [1636, 'entailment'], [1637, 'contradiction'], [1638, 'contradiction'], [1639, 'entailment'], [1640, 'entailment'], [1641, 'entailment'], [1642, 'entailment'], [1643, 'entailment'], [1644, 'contradiction'], [1645, 'neutral'], [1646, 'contradiction'], [1647, 'entailment'], [1648, 'entailment'], [1649, 'contradiction'], [1650, 'contradiction'], [1651, 'entailment'], [1652, 'neutral'], [1653, 'contradiction'], [1654, 'contradiction'], [1655, 'entailment'], [1656, 'neutral'], [1657, 'contradiction'], [1658, 'neutral'], [1659, 'neutral'], [1660, 'entailment'], [1661, 'neutral'], [1662, 'entailment'], [1663, 'neutral'], [1664, 'neutral'], [1665, 'neutral']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
        "\n",
        "df.to_csv('./result/submission.csv', index=False)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qbJeQityu7i",
        "outputId": "e31486d9-e30c-48fb-8592-7e4d8757a019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      index          label\n",
            "0         0  contradiction\n",
            "1         1        neutral\n",
            "2         2     entailment\n",
            "3         3  contradiction\n",
            "4         4  contradiction\n",
            "...     ...            ...\n",
            "1661   1661        neutral\n",
            "1662   1662     entailment\n",
            "1663   1663        neutral\n",
            "1664   1664        neutral\n",
            "1665   1665        neutral\n",
            "\n",
            "[1666 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}
