{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/KLUE/blob/main/220222_sm_TPU_kfold_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W2Q9Qbnlz_t"
      },
      "source": [
        "# A full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCra33qIlz_1"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee9BbI2znxA0",
        "outputId": "ff697a09-1609-4636-e36a-716d8b508c3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31KhLlUBlz_2",
        "outputId": "58ad1b55-ef2a-4dd3-e755-372107e0fc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 86.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 67.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 98.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 93.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 82.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 82.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 81.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 87.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 87.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, huggingface-hub, fsspec, aiohttp, xxhash, transformers, sentencepiece, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.3 frozenlist-1.3.0 fsspec-2022.1.0 huggingface-hub-0.4.0 multidict-6.0.2 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.5 transformers-4.16.2 xxhash-2.0.2 yarl-1.7.2\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.10.0.2)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.5.1\n",
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 18 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████▉                           | 125.5 MB 1.2 MB/s eta 0:09:33"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "_vxmupBjnvqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModel,AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
        "import gc\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_metric\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from accelerate import notebook_launcher"
      ],
      "metadata": {
        "id": "R7yZowtOokD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kakao_snli = pd.read_csv('/content/drive/MyDrive/KLUE/snli_1.0_train.ko.tsv', sep='\\t', encoding='utf-8')\n",
        "kakao_snli = kakao_snli[:100000]\n",
        "\n",
        "PATH =  '/content/drive/MyDrive/KLUE'\n",
        "test = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
        "\n",
        "kakao_dev = pd.read_csv('/content/drive/MyDrive/KLUE/xnli.dev.ko.tsv', sep='\\t', encoding='utf-8')\n",
        "kakao_test = pd.read_csv('/content/drive/MyDrive/KLUE/xnli.test.ko.tsv', sep='\\t', encoding='utf-8')\n",
        "klue_dev = pd.read_json('/content/drive/MyDrive/KLUE/klue-nli-v1.1_dev.json')\n",
        "klue_train = pd.read_json('/content/drive/MyDrive/KLUE/klue-nli-v1.1_train.json')\n",
        "temp = pd.read_csv(os.path.join(PATH, 'train_data.csv'), encoding='utf-8')\n",
        "\n",
        "klue_dev = klue_dev[['premise', 'hypothesis', 'gold_label']]\n",
        "klue_train = klue_train[['premise', 'hypothesis', 'gold_label']]\n",
        "klue_dev.rename(columns = {'gold_label':'label'}, inplace=True)\n",
        "klue_train.rename(columns = {'gold_label':'label'}, inplace=True)\n",
        "\n",
        "train = pd.concat([kakao_dev,kakao_test, kakao_snli])\n",
        "train.rename(columns = {'sentence1':'premise','sentence2':'hypothesis','gold_label':'label'},inplace=True)\n",
        "train = pd.concat([temp, train, klue_dev, klue_train], axis=0)\n",
        "train = train.reset_index(drop=True)\n",
        "train['index'] = train.index\n",
        "train = train.dropna()\n",
        "train"
      ],
      "metadata": {
        "id": "PpIow7U5n0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGWbl8edlz_5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'klue/roberta-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=False)\n",
        "\n",
        "# tokenized_train = tokenizer(\n",
        "#     list(train_dataset['premise']),\n",
        "#     list(train_dataset['hypothesis']),\n",
        "#     return_tensors=\"pt\",\n",
        "#     max_length=128, # Max_Length = 190\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     add_special_tokens=True\n",
        "# )\n",
        "\n",
        "# tokenized_eval = tokenizer(\n",
        "#     list(eval_dataset['premise']),\n",
        "#     list(eval_dataset['hypothesis']),\n",
        "#     return_tensors=\"pt\",\n",
        "#     max_length=128,\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "#     add_special_tokens=True\n",
        "# )"
      ],
      "metadata": {
        "id": "L32MUBMXocy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['labels'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "J3OrqrY3or6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_num(label):\n",
        "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(label_dict[v])\n",
        "    \n",
        "    return num_label\n",
        "\n",
        "# train_label = label_to_num(train['label'].values)\n",
        "# eval_label = label_to_num(eval_dataset['label'].values)"
      ],
      "metadata": {
        "id": "bqgw3AKZotnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "# eval_dataset = BERTDataset(tokenized_eval, eval_label)"
      ],
      "metadata": {
        "id": "O9oWnK57owKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset[0]"
      ],
      "metadata": {
        "id": "V-LGu7eI9cDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "bs0VFrWuA_6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djc4iH0nl0AA"
      },
      "outputs": [],
      "source": [
        "# def training_function():\n",
        "#   accelerator = Accelerator()\n",
        "\n",
        "#   train_dataloader = torch.utils.data.DataLoader(\n",
        "#                     train_dataset, \n",
        "#                     batch_size=16, sampler=train_subsampler, num_workers=2)\n",
        "#   eval_dataloader = torch.utils.data.DataLoader(\n",
        "#                     train_dataset,\n",
        "#                     batch_size=16, sampler=test_subsampler, num_workers=2)\n",
        "\n",
        "#   config = AutoConfig.from_pretrained(checkpoint)\n",
        "#   config.num_labels = 3\n",
        "#   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "#   optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "#   model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "#   num_epochs = 5\n",
        "#   num_training_steps = num_epochs * len(train_dataloader)\n",
        "#   progress_bar = tqdm(range(num_training_steps))\n",
        "#   lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "#       optimizer=optimizer,\n",
        "#       num_warmup_steps=1,\n",
        "#       num_training_steps=num_training_steps,\n",
        "#   )\n",
        "\n",
        "#   for epoch in range(num_epochs):\n",
        "#       train_acc = 0.0\n",
        "#       test_acc = 0.0\n",
        "\n",
        "#       model.train()\n",
        "#       for batch_id, batch in enumerate(train_dataloader):\n",
        "#           outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "#           loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "#           accelerator.backward(loss)\n",
        "\n",
        "#           optimizer.step()\n",
        "#           lr_scheduler.step()\n",
        "#           optimizer.zero_grad()\n",
        "#           progress_bar.update(1)\n",
        "#           train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "#       print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "#       model.eval()\n",
        "#       for batch_id, batch in enumerate(eval_dataloader):\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "\n",
        "#         test_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "#       print(\"epoch {} test acc {}\".format(epoch+1, test_acc / (batch_id+1)))\n",
        "#       gc.collect()\n",
        "#   accelerator.wait_for_everyone()\n",
        "#   unwrapped_model = accelerator.unwrap_model(model)\n",
        "#   accelerator.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/KLUE/model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# Start print\n",
        "print('--------------------------------')\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "def training_function():\n",
        "  kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "  print('--------------------------------')\n",
        "\n",
        "  for fold, (train_ids, test_ids) in enumerate(kfold.split(train, train['label'])):\n",
        "    print('')\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "\n",
        "    train_label = label_to_num(train['label'].values)\n",
        "\n",
        "    tokenized_train = tokenizer(\n",
        "      list(train['premise']),\n",
        "      list(train['hypothesis']),\n",
        "      return_tensors=\"pt\",\n",
        "      max_length=128, # Max_Length = 190\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True\n",
        "      )\n",
        "    \n",
        "    train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "\n",
        "    # def training_function():\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=16, sampler=train_subsampler, num_workers=2)\n",
        "    eval_dataloader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=16, sampler=test_subsampler, num_workers=2)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(checkpoint)\n",
        "    config.num_labels = 3\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=\t1e-5)\n",
        "\n",
        "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
        "\n",
        "    num_epochs = 5\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=1,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc = 0.0\n",
        "        test_acc = 0.0\n",
        "\n",
        "        model.train()\n",
        "        for batch_id, batch in enumerate(train_dataloader):\n",
        "            outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "            loss = F.cross_entropy(outputs[0], batch['labels'])\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "            train_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "        print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "        model.eval()\n",
        "        for batch_id, batch in enumerate(eval_dataloader):\n",
        "          with torch.no_grad():\n",
        "              outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])\n",
        "\n",
        "          test_acc += calc_accuracy(outputs.logits, batch['labels'])\n",
        "        print(\"epoch {} test acc {}\".format(epoch+1, test_acc / (batch_id+1)))\n",
        "        gc.collect()\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    accelerator.save(unwrapped_model.state_dict(), '/content/drive/MyDrive/KLUE/model' + str(fold) + '.pt')"
      ],
      "metadata": {
        "id": "iw581lfmF_J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_launcher(training_function)"
      ],
      "metadata": {
        "id": "10duXIf_QYhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(checkpoint)\n",
        "config.num_labels = 3\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)"
      ],
      "metadata": {
        "id": "xje6ZDJJkmZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerator()\n",
        "model = accelerator.unwrap_model(model)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model/model.pt'))"
      ],
      "metadata": {
        "id": "zHuUP_vQc116"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "efirdTEK_c9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = label_to_num(test['label'].values)\n",
        "\n",
        "tokenized_test = tokenizer(\n",
        "    list(test['premise']),\n",
        "    list(test['hypothesis']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "test_dataset = BERTDataset(tokenized_test, test_label)"
      ],
      "metadata": {
        "id": "XI5RYcMl_eYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "output_pred = []\n",
        "output_prob = []\n",
        "\n",
        "model, dataloader= accelerator.prepare(model, dataloader)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(tqdm(dataloader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=data['input_ids'],\n",
        "            attention_mask=data['attention_mask'],\n",
        "            token_type_ids=data['token_type_ids']\n",
        "        )\n",
        "    logits = outputs[0]\n",
        "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits, axis=-1)\n",
        "\n",
        "    output_pred.append(result)\n",
        "    output_prob.append(prob)\n",
        "  \n",
        "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
        "print(pred_answer)"
      ],
      "metadata": {
        "id": "DLTqv229_gRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_label(label):\n",
        "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
        "    str_label = []\n",
        "\n",
        "    for i, v in enumerate(label):\n",
        "        str_label.append([i,label_dict[v]])\n",
        "    \n",
        "    return str_label\n",
        "\n",
        "answer = num_to_label(pred_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "NABEttQw_h3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/KLUE/submission.csv', index=False)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "6rV5-ld6_i74"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "A full training",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
